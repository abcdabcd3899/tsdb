create table cost_agg_t1(a int, b int, c int);
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into cost_agg_t1 select i, random() * 99999, i % 2000 from generate_series(1, 1000000) i;
create table cost_agg_t2 as select * from cost_agg_t1 with no data;
NOTICE:  Table doesn't have 'DISTRIBUTED BY' clause -- Using column(s) named 'a' as the Greenplum Database data distribution key for this table.
HINT:  The 'DISTRIBUTED BY' clause determines the distribution of data. Make sure column(s) chosen are the optimal data distribution key to minimize skew.
insert into cost_agg_t2 select i, random() * 99999, i % 300000 from generate_series(1, 1000000) i;
--
-- Test planner's decisions on aggregates when only little memory is available.
--
set statement_mem= '1800 kB';
-- There are only 2000 distinct values of 'c' in the table, which fits
-- comfortably in an in-memory hash table.
explain select avg(b) from cost_agg_t1 group by c;
                                               QUERY PLAN                                               
--------------------------------------------------------------------------------------------------------
 Gather Motion 9:1  (slice1; segments: 9)  (cost=3955.33..3980.33 rows=2000 width=36)
   ->  Finalize HashAggregate  (cost=3955.33..3958.11 rows=222 width=36)
         Group Key: c
         ->  Redistribute Motion 9:9  (slice2; segments: 9)  (cost=3885.33..3945.33 rows=2000 width=36)
               Hash Key: c
               ->  Partial HashAggregate  (cost=3885.33..3905.33 rows=2000 width=36)
                     Group Key: c
                     ->  Parallel Seq Scan on cost_agg_t1  (cost=0.00..3190.89 rows=138889 width=8)
 Optimizer: Postgres query optimizer
(9 rows)

-- In the other table, there are 300000 distinct values of 'c', which doesn't
-- fit in statement_mem. The planner chooses to do a single-phase agg for this.
--
-- In the single-phase plan, the aggregation is performed after redistrbuting
-- the data, which means that each node only has to process 1/(# of segments)
-- fraction of the data. That fits in memory, whereas an initial stage before
-- redistributing would not. And it would eliminate only a few rows, anyway.
explain select avg(b) from cost_agg_t2 group by c;
                                              QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
 Gather Motion 9:1  (slice1; segments: 9)  (cost=6663.11..10269.20 rows=288487 width=36)
   ->  HashAggregate  (cost=6663.11..7063.79 rows=32054 width=36)
         Group Key: c
         Planned Partitions: 4
         ->  Redistribute Motion 9:9  (slice2; segments: 9)  (cost=0.00..5968.67 rows=138889 width=8)
               Hash Key: c
               ->  Parallel Seq Scan on cost_agg_t2  (cost=0.00..3190.89 rows=138889 width=8)
 Optimizer: Postgres query optimizer
(8 rows)

-- But if there are a lot more duplicate values, the two-stage plan becomes
-- cheaper again, even though it doesn't git in memory and has to spill.
insert into cost_agg_t2 select i, random() * 99999,1 from generate_series(1, 200000) i;
analyze cost_agg_t2;
explain select avg(b) from cost_agg_t2 group by c;
                                              QUERY PLAN                                              
------------------------------------------------------------------------------------------------------
 Gather Motion 9:1  (slice1; segments: 9)  (cost=7996.33..9286.75 rows=103233 width=36)
   ->  HashAggregate  (cost=7996.33..8139.71 rows=11470 width=36)
         Group Key: c
         ->  Redistribute Motion 9:9  (slice2; segments: 9)  (cost=0.00..7163.00 rows=166667 width=8)
               Hash Key: c
               ->  Parallel Seq Scan on cost_agg_t2  (cost=0.00..3829.67 rows=166667 width=8)
 Optimizer: Postgres query optimizer
(7 rows)

drop table cost_agg_t1;
drop table cost_agg_t2;
reset statement_mem;
